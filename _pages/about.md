---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a master student at [Group of Networked Sensing and Control (NesC)](http://nesc.zju.edu.cn/#/), Zhejiang University, advised by [Prof. Chaojie Gu](https://chaojiegu.github.io/) and [Prof. Shibo He](https://person.zju.edu.cn/en/shibohe).
I currently also work as a visiting student with [Prof. Rui Tan](https://personal.ntu.edu.sg/tanrui/), Nangyang Technological University.
I received my B.E. degree from Control Science and Engineering College, Zhejiang University, China in 2021.

<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->
&nbsp;

<!-- My research interest is **mobile sensing and AIoT (AI + Internet of Things)**, with a primary focus on related fields of developing artificial intelligence sensing systems for human's daily lives.
My current research focuses on wireless-based sensing and wearable-based sensing, including human activity recognition, authentication, etc. -->

I am broadly interested in **mobile sensing and AIoT** (AI + Internet of Things), with a primary focus on related fields of developing artificial intelligence sensing systems for practical applications, including human activity recognition, authentication, etc.

[[CV](Lilin_CV_2310.pdf)]

# News
- [2023.10] &nbsp; One workshop paper accepted to SenSys 2023.
- [2023.09] &nbsp; One paper is conditionally accepted by SenSys 2023.
- [2023.06] &nbsp; One paper is accpected by IEEE Transactions on Circuits and Systems for Video Technology. 

# Publications 

<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div> -->

<!-- [MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels]() -->
MESEN: Exploit Multimodal Data to Design Unimodal Human Activity Recognition with Few Labels

**Lilin Xu**, Chaojie Gu, Rui Tan, Shibo He, Jiming Chen

The 21th ACM Conference on Embedded Networked Sensor Systems (**SenSys 2023**).

(Acceptance ratio: 34/179=19.0%)

<!-- [SenSys 2023]
[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

&nbsp;

[Latency-aware Neural Architecture Performance Predictor with Query-to-Tier Technique](https://ieeexplore.ieee.org/document/10155437)

Bicheng Guo\*, **Lilin Xu**\* (*technically equal contribution), Tao Chen, Peng Ye, Shibo He, Haoyu Liu, Jiming Chen

IEEE Transactions on Circuits and Systems for Video Technology (**TCSVT**, 2023)

[[**Paper**](TCSVT_NAR.pdf)]

<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

&nbsp;

Work in Progress: Enabling User Identification for mmWave-based Gesture Recognition Systems

**Lilin Xu**, Keyi Wang, Chaojie Gu*, Shibo He, Jiming Chen

The first ACM workshop on mmWave sensing systems and applications (**SenSys Workshop ACM mmWaveSys 2023**)


# Selected Awards
- Zhejiang University Award of Honor for Graduate, 2023.
- Zhejiang University Award of Honor for Graduate, 2022.
- AI Studio 2022 CVPR Track2: Performance Estimation Track, Top 10 Award (8/190), 2022. 
- College Academic Excellence First-prize Scholarship, 2022. 
- Zhejiang University Second-prize Scholarship, 2020. 
- Zhejiang University First-prize Scholarship (Top 3%), 2019.
- Zhejiang University Second-prize Scholarship, 2018. 

# Educations
- *2021.09 - 2024.03 (Expected)*, Master, [Group of Networked Sensing and Control (NesC)](http://nesc.zju.edu.cn/#/), Zhejiang University, Hangzhou. (GPA: 3.91/4.0)
- *2017.09 - 2021.06*, Undergraduate, Control Science and Engineering College, Zhejiang Univeristy, Hangzhou. (GPA: 3.95/4.0, Rank: 5/120)

<!-- # ðŸ’¬ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

# Visiting Experience
- *2023.04 - present*, Visiting Research Student, [NTU IoT Research Group](https://ntuiot.xyz/), Nangyang Technological University, Singapore.